# Colorful Image Colorization

- Colorful Image Colorization 原论文[链接](https://arxiv.org/abs/1603.08511)
- Github[链接](https://github.com/nilboy/colorization-tf)
 
论文主要提出一种**使用深度学习对黑白图片进行重新着色**的方法

注意，输入图像为一个`256x256x1`的图像，为什么是1呢,因为黑白图像只有一层通道，作者在这里将所有的图像映射到了LAB颜色空间，以下所有有关通道的内容，全部基于LAB通道。

由于LAB颜色空间的通道L代表明度，取值范围(0, 100), L通道能够很好的描述一个像素的灰度值，因此，只需把输入的黑白图像当做最后生成图像的L通道就好了，我们要做的工作其实是补全a通道和b通道。

![Screen Shot 2017-04-20 at 3.53.41 P](media/Screen%20Shot%202017-04-20%20at%203.53.41%20PM.png)

网络主要结构如上图所示，使用一个类似于VGG的网络（filter size为3的小卷积核卷积，使用同样数量的卷积核卷积多个Block），最终得到一个64x64x313的特征图。

64可以理解，就是特征图的大小嘛，但是313如何理解？我们知道，LAB颜色空间的a和b取值在-127到127之间，阅读原文可以发现，论文中使用了类似于`多类别分类`的方法进行训练，我们要预测的是图像中某个像素的(a, b)通道的组成的pair，那么比较直观的方法是：

- 使神经网路输出两个值(a, b)
- 把a,b当做两个输出的回归问题来对待
- 直接优化回归的loss

但是作者使用了多分类的方法如何操作呢？主要方法是将图像的(a, b)通道装箱,以`10`为bin size，这样把-127,127的空间划分为更少的类别，比如`[-127,-117)`是一个bin,那么在这个区间内的所有像素，都被划分为同一个类别，把这个思想应用到二维空间，就相当于将整个平面划分成了许多网格，如图所示：

![Screen Shot 2017-04-20 at 4.16.48 P](media/Screen%20Shot%202017-04-20%20at%204.16.48%20PM.png)
图中可见部分共有313个grid，这个地方一直弄不明白，为什么不是全部覆盖网格，而是只有中间的部分。每个grid是一个类别，那么现在，预测(a, b)值对的问题变成了一个313个类别的分类问题(类似于ImageNet比赛共分为1000个类别一样)，然后就走上多分类问题的正常轨道了：
 
 - 输入图片
 - 使用上面类VGG网络Forward得到一个(64x64x313)的特征图
 - 将训练图片(256x256x3)使用stride=4下采样，得到(64x64x3)图像
 - 将下采样后的图像的每个像素的(a, b)对转换为313，这样每个像素都变成一维了(64x64x1)
 - 将313类别通过onehot编码转换为(64x64x313)大小
 - 转换后的矩阵与神经网路的输出矩阵做cross-entropy loss
 - 反向传播，开始训练

当训练结束以后，如何对黑白图像进行染色呢？

- 输入数据(256x256x1)直接当做L通道下采样后得到(64x64x1)图像A
- 输入数据投入网络得到(64x64x313)输出，转为(4096x313)大小
- 将转换后的矩阵乘以(313x2)的转换矩阵得到(4096x2)的矩阵，对应每隔像素的(a, b)值
- resize得到(64x64x2)的图像B
- 将AB图像拼接，得到(64x64x3)图像
- 图像转换为RGB，并放大到(256x256x3)，到此结束

可以看到，其实在着色过程中，图像质量先压缩再放大，图像的清晰度有所损失，不过效果还可以~

### 总结
这篇文章算是一个DL的有趣的应用吧，还挺有意思的，文章的思想中，将AB对离散化，变成了313个分离的变量，这种思路还是很有借鉴价值的，通过这种方式，可以用多分类深度学习来解决多元回归问题（当回归结果维数较低的时候）






